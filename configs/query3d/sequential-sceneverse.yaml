name: "Sequential-Grounding"
rng_seed: 42 
num_gpu: 1
mode: "train" 
note: ""
base_dir: ""
exp_dir: ""

resume: False

debug:
  flag: False
  hard_debug: False
  debug_size: 4

logger:
  name: "wandb"
  entity: TBD
  autoname: True

# dataset details
data:
  load_scan_options:
    load_inst_info: True
    load_pc_info: True
    load_segment_info: False
    load_image_segment_feat: False
    load_point_segment_feat: False
    load_image_obj_feat: False
    load_voxel_obj_feat: False
  process_num: 0
  scene_verse_base: TBD
  scene_verse_aux: TBD
  scene_verse_pred: TBD
  sequential_grounding_base: TBD
  multi_step_context: True
  drop_data_percent: 0.0

  train: [SequentialGroundingScanNet, SequentialGrounding3RScan, SequentialGroundingMultiScan, SequentialGroundingARKitScenes, SequentialGroundingHM3D]
  val: ${data.train}
  test: ${data.train}
  SequentialGroundingScanNet:
    pc_type: 'gt'
    evaluator: "SequentialGroundingEval_GroundFlow"
  SequentialGrounding3RScan:
    pc_type: 'gt'
    evaluator: "SequentialGroundingEval_GroundFlow"
  SequentialGroundingMultiScan:
    pc_type: 'gt'
    evaluator: "SequentialGroundingEval_GroundFlow"
  SequentialGroundingARKitScenes:
    pc_type: 'gt'
    evaluator: "SequentialGroundingEval_GroundFlow"
  SequentialGroundingHM3D:
    pc_type: 'gt'
    evaluator: "SequentialGroundingEval_GroundFlow"

task: 'Query3D'
data_wrapper:
  train: 'UnifiedTaskDatasetWrapper'
  val: ${data_wrapper.train}
  test: ${data_wrapper.train}
  tokenizer: openai/clip-vit-large-patch14
  generation_tokenizer: t5-small

# Training details
trainer: "MultitaskTrainer"
ckpt_path: ""
pretrain_ckpt_path: ""

# dataloader details
dataloader:
  batchsize: 32
  num_workers: 5
  balance_dataset: False
  filter_empty_annotations: False

solver:
  gradient_accumulation_steps: 1
  lr: 1e-4
  grad_norm: 5.0
  optim:
    name: "AdamW"
    args:
      betas: [0.9, 0.98]
  sched:
    name: "warmup_cosine"
    args:
      warmup_steps: 5000
  epochs: 50 
  epochs_per_eval: 1
  epochs_per_save: 0

eval:
  save: False

# Model details

model:
  name: Query3DUnified_GroundFlow
  notes: exp_1
  memories: [pc, prompt]
  hidden_size: 768
  use_offline_voxel_fts: True
  use_offline_attn_mask: False
  skip_query_encoder_mask_pred: True
  obj_loc:
    spatial_dim: 5
    dim_loc: 6
    pairwise_rel_type: "center"
  txt_encoder:
    name: CLIPLanguageEncoder
    args:
      use_projection: True
      projection_type: "mlp"
      num_projection_layers: 1
  mv_encoder:
    name: ObjectEncoder
    args:
      input_feat_size : 768
      hidden_size: ${model.hidden_size}
      use_projection: True
      dropout: 0.1
      use_cls_head: False
  voxel_encoder:
    name: ObjectEncoder
    args:
      input_feat_size : 64
      hidden_size: ${model.hidden_size}
      use_projection: True
      dropout: 0.1
      use_cls_head: False
  pc_encoder:
    name: ObjectEncoder
    args:
      backbone: "pointnet++"
      freeze_backbone: True
      hidden_size: ${model.hidden_size}
      dropout: 0.1
      pretrained: TBD
      use_cls_head: False
  unified_encoder:
    name: "QueryMaskEncoder_GroundFlow" 
    args:
      hidden_size: ${model.hidden_size}
      num_attention_heads: 12
      num_layers: 4
      spatial_selfattn: True
      memories: ${model.memories}
      drop_memories_test: []
      memory_dropout: 0.6
      structure: "mixed"
      use_self_mask: False
      num_blocks: 1

  heads: ["ground"]
  ground_head:
    name: "GroundHead"
    args:
      hidden_size: 384
      input_size: ${model.hidden_size}
      dropout: 0.3
  generation_head:
    name: "T5"
    args:
      variant: t5-small
      input_size: ${model.hidden_size}
      use_projection: True
      max_new_tokens: 50 
    lr: 1e-5

  loss_list: [ground_loss]
  vis_loss_list: []
  loss_weights: {'ground_loss': 10}

  